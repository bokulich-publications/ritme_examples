{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e8241c71",
      "metadata": {},
      "source": [
        "# Evaluate *ritme* trials of all usecases\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1aea4401",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b08cb4f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "import pandas as pd\n",
        "\n",
        "from src.evaluate_trials import (\n",
        "    plot_complexity_vs_metric,\n",
        "    plot_trend_over_time,\n",
        "    boxplot_metric,\n",
        "    multi_boxplot_metric,\n",
        ")\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "361c1df1",
      "metadata": {},
      "outputs": [],
      "source": [
        "######## USER INPUTS ########\n",
        "\n",
        "# path to extracted MLflow logs - with script extract_all_logs.sh\n",
        "log_folder_location = \"merged_u1_u2_runs_work_no2trac.csv\"\n",
        "\n",
        "# which usecase to analyze: \"u1\", \"u2\", \"u3\" or \"all\"\n",
        "usecase = \"u1\"\n",
        "\n",
        "# which samplers to analyse: \"tpe\", \"random\" or \"all\"\n",
        "sampler = \"tpe\"\n",
        "# whether to only analyze one model type or all (=\"all\")\n",
        "model_type = \"all\"\n",
        "\n",
        "# how many trials to consider for complexity vs. performance plot\n",
        "top_x = 1000\n",
        "\n",
        "# figure saving dpi\n",
        "dpi = 400\n",
        "######## END USER INPUTS #####"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31db9eea",
      "metadata": {},
      "outputs": [],
      "source": [
        "# set title\n",
        "if usecase == \"u1\":\n",
        "    title = \"Usecase 1\"\n",
        "    best_model_type = \"xgb\"\n",
        "elif usecase == \"u2\":\n",
        "    title = \"Usecase 2\"\n",
        "    best_model_type = \"linreg\"\n",
        "elif usecase == \"u3\":\n",
        "    title = \"Usecase 3\"\n",
        "    best_model_type = \"xgb\"\n",
        "else:\n",
        "    title = \"All usecases\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8390260",
      "metadata": {},
      "source": [
        "## Extract trial information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3347709",
      "metadata": {},
      "outputs": [],
      "source": [
        "# extract all trial information\n",
        "all_trials = pd.read_csv(log_folder_location)\n",
        "# sort by asc metrics.rmse_val\n",
        "all_trials = all_trials.sort_values(by=\"metrics.rmse_val\", ascending=True)\n",
        "print(f\"Found {all_trials.shape[0]} trials\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56e806e7",
      "metadata": {},
      "outputs": [],
      "source": [
        "if usecase != \"all\":\n",
        "    print(f\"Analyzing trials for usecase: {usecase}\")\n",
        "    all_trials = all_trials[all_trials[\"tags.experiment_tag\"].str.startswith(usecase)]\n",
        "\n",
        "if sampler != \"all\":\n",
        "    print(f\"Analyzing trials for sampler: {sampler}\")\n",
        "    all_trials = all_trials[all_trials[\"tags.experiment_tag\"].str.contains(sampler)]\n",
        "\n",
        "if model_type != \"all\":\n",
        "    print(f\"Analyzing trials for model type: {model_type}\")\n",
        "    all_trials = all_trials[all_trials[\"params.model\"] == model_type]\n",
        "\n",
        "print(f\"Selected {all_trials.shape[0]} trials\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e01f375",
      "metadata": {},
      "source": [
        "## Insights on performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a9e4153",
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = multi_boxplot_metric(\n",
        "    all_trials,\n",
        "    metric_col=\"metrics.rmse_val\",\n",
        "    metric_name=\"RMSE Validation\",\n",
        "    group_specs=[\n",
        "        (\"params.data_aggregation\", \"Data aggregation\"),\n",
        "        (\"params.data_selection\", \"Data selection\"),\n",
        "        (\"params.data_transform\", \"Data transform\"),\n",
        "        (\"params.data_enrich\", \"Data enrichment\"),\n",
        "        (\"params.model\", \"Model type\"),\n",
        "    ],\n",
        "    order_by_median=True,\n",
        "    showfliers=False,\n",
        "    title=title,\n",
        ")\n",
        "fig.savefig(\n",
        "    f\"result_figures/boxplot_all_trials_{usecase}.pdf\", bbox_inches=\"tight\", dpi=dpi\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91e71069",
      "metadata": {},
      "source": [
        "## Model complexity vs. performance: top X trials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed85e8a6",
      "metadata": {},
      "outputs": [],
      "source": [
        "top_x_trials = all_trials.head(top_x)\n",
        "top_x_trials[\"params.model\"].value_counts()\n",
        "figc, _ = plot_complexity_vs_metric(\n",
        "    top_x_trials,\n",
        "    metric_col=\"metrics.rmse_val\",\n",
        "    metric_name=\"RMSE Validation\",\n",
        "    group_col=\"params.model\",\n",
        "    group_name=\"Model type\",\n",
        "    n=top_x,\n",
        "    figsize=(7, 6),\n",
        "    title=title,\n",
        ")\n",
        "\n",
        "figc.savefig(\n",
        "    f\"result_figures/complexity_all_trials_{usecase}.pdf\", bbox_inches=\"tight\", dpi=dpi\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26675503",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Training over time\n",
        "\n",
        "# metric = \"rmse_val\"\n",
        "# for model in all_trials[\"params.model\"].unique():\n",
        "#     print(f\"Plotting trend for model: {model}\")\n",
        "#     model_trials = all_trials[all_trials[\"params.model\"] == model]\n",
        "#     plot_trend_over_time(\n",
        "#         model_trials, f\"metrics.{metric}\", window=100, title_prefix=f\"Model: {model}\", figsize=(12, 6\n",
        "#     ))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8319796a",
      "metadata": {},
      "source": [
        "# Top 1 trial\n",
        "\n",
        "based on held-out test set performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "798cc0b8",
      "metadata": {},
      "outputs": [],
      "source": [
        "top_1_trial = all_trials.loc[all_trials[\"params.model\"] == best_model_type, :].head(1)\n",
        "\n",
        "top_1_trial[\"metrics.nb_features\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b26d20c",
      "metadata": {},
      "outputs": [],
      "source": [
        "top_1_trial[\"tags.experiment_tag\"]"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ritme_model",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
