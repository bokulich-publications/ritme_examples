{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e8241c71",
      "metadata": {},
      "source": [
        "# Evaluate *ritme* trials of all usecases\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1aea4401",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b08cb4f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from src.evaluate_trials import (\n",
        "    boxplot_metric,\n",
        "    multi_boxplot_metric,\n",
        "    plot_complexity_vs_metric,\n",
        "    plot_trend_over_time,\n",
        "    plot_trend_over_time_multi_models,\n",
        ")\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "361c1df1",
      "metadata": {},
      "outputs": [],
      "source": [
        "######## USER INPUTS ########\n",
        "\n",
        "# path to extracted MLflow logs - with script extract_all_logs.sh\n",
        "log_folder_location = \"merged_all_trials.csv\"\n",
        "\n",
        "# which usecase to analyze: \"u1\", \"u2\", \"u3\" or \"all\"\n",
        "usecase = \"u2\"\n",
        "\n",
        "# which samplers to analyse: \"tpe\", \"random\"\n",
        "sampler = \"random\"\n",
        "\n",
        "# how many trials to consider for complexity vs. performance plot\n",
        "top_x = 1000\n",
        "\n",
        "# figure saving dpi\n",
        "dpi = 400\n",
        "######## END USER INPUTS #####"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31db9eea",
      "metadata": {},
      "outputs": [],
      "source": [
        "# set title\n",
        "if usecase == \"u1\":\n",
        "    title = \"Usecase 1\"\n",
        "    best_model_type = \"xgb\"\n",
        "    log_x_scale = False\n",
        "elif usecase == \"u2\":\n",
        "    title = \"Usecase 2\"\n",
        "    best_model_type = \"linreg\"\n",
        "    log_x_scale = False\n",
        "elif usecase == \"u3\":\n",
        "    title = \"Usecase 3\"\n",
        "    best_model_type = \"xgb\"\n",
        "    log_x_scale = True\n",
        "else:\n",
        "    title = \"All usecases\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8390260",
      "metadata": {},
      "source": [
        "## Extract trial information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3347709",
      "metadata": {},
      "outputs": [],
      "source": [
        "# extract all trial information\n",
        "all_trials = pd.read_csv(log_folder_location)\n",
        "# sort by asc metrics.rmse_val\n",
        "all_trials = all_trials.sort_values(by=\"metrics.rmse_val\", ascending=True)\n",
        "print(f\"Found {all_trials.shape[0]} trials\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56e806e7",
      "metadata": {},
      "outputs": [],
      "source": [
        "if usecase != \"all\":\n",
        "    print(f\"Analyzing trials for usecase: {usecase}\")\n",
        "    if usecase == \"u3\":\n",
        "        all_trials = all_trials[\n",
        "            np.logical_and(\n",
        "                all_trials[\"tags.experiment_tag\"].str.startswith(\"u3_galaxy\"),\n",
        "                ~all_trials[\"tags.experiment_tag\"].str.contains(\"w_start\"),\n",
        "            )\n",
        "        ]\n",
        "    else:\n",
        "        all_trials = all_trials[\n",
        "            all_trials[\"tags.experiment_tag\"].str.startswith(usecase)\n",
        "        ]\n",
        "\n",
        "if sampler != \"all\":\n",
        "    print(f\"Analyzing trials for sampler: {sampler}\")\n",
        "    all_trials = all_trials[all_trials[\"tags.experiment_tag\"].str.contains(sampler)]\n",
        "\n",
        "print(f\"Selected {all_trials.shape[0]} trials\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72801522",
      "metadata": {},
      "outputs": [],
      "source": [
        "# find the best trial\n",
        "top_1_trial = all_trials.head(1)\n",
        "top_1_trial[\"tags.experiment_tag\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e01f375",
      "metadata": {},
      "source": [
        "## Insights on performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a9e4153",
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = multi_boxplot_metric(\n",
        "    all_trials,\n",
        "    metric_col=\"metrics.rmse_val\",\n",
        "    metric_name=\"RMSE Validation\",\n",
        "    group_specs=[\n",
        "        (\"params.data_aggregation\", \"Data aggregation\"),\n",
        "        (\"params.data_selection\", \"Data selection\"),\n",
        "        (\"params.data_transform\", \"Data transform\"),\n",
        "        (\"params.data_enrich\", \"Data enrichment\"),\n",
        "        (\"params.model\", \"Model type\"),\n",
        "    ],\n",
        "    order_by_median=True,\n",
        "    showfliers=False,\n",
        "    title=title,\n",
        "    x_log_scale=log_x_scale,\n",
        ")\n",
        "fig.savefig(\n",
        "    f\"result_figures/boxplot_all_trials_{usecase}_{sampler}.pdf\",\n",
        "    bbox_inches=\"tight\",\n",
        "    dpi=dpi,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91e71069",
      "metadata": {},
      "source": [
        "## Model complexity vs. performance: top X trials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4128255c",
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = multi_boxplot_metric(\n",
        "    all_trials,\n",
        "    metric_col=\"metrics.nb_features\",\n",
        "    metric_name=\"Number of features\",\n",
        "    group_specs=[\n",
        "        (\"params.model\", \"Model type\"),\n",
        "    ],\n",
        "    order_by_median=True,\n",
        "    showfliers=False,\n",
        "    title=title,\n",
        "    x_log_scale=True,\n",
        "    figsize=(6, 4),\n",
        ")\n",
        "plt.tight_layout()\n",
        "fig.savefig(\n",
        "    f\"result_figures/boxplot_all_trials_{usecase}_{sampler}_nb_fts.pdf\",\n",
        "    bbox_inches=\"tight\",\n",
        "    dpi=dpi,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed85e8a6",
      "metadata": {},
      "outputs": [],
      "source": [
        "top_x_trials = all_trials.head(top_x)\n",
        "top_x_trials[\"params.model\"].value_counts()\n",
        "figc, _ = plot_complexity_vs_metric(\n",
        "    top_x_trials,\n",
        "    metric_col=\"metrics.rmse_val\",\n",
        "    metric_name=\"RMSE Validation\",\n",
        "    group_col=\"params.model\",\n",
        "    group_name=\"Model type\",\n",
        "    n=top_x,\n",
        "    figsize=(7, 6),\n",
        "    title=title,\n",
        "    x_log_scale=True,\n",
        ")\n",
        "\n",
        "figc.savefig(\n",
        "    f\"result_figures/complexity_top_trials_{usecase}_{sampler}.pdf\",\n",
        "    bbox_inches=\"tight\",\n",
        "    dpi=dpi,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6938f10b",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"number of trials w random scheduler: {0.2 * 2500}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26675503",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Training over time\n",
        "\n",
        "# metric = \"rmse_val\"\n",
        "# for model in all_trials[\"params.model\"].unique():\n",
        "#     model_trials = all_trials[all_trials[\"params.model\"] == model]\n",
        "#     plot_trend_over_time(\n",
        "#         model_trials,\n",
        "#         f\"metrics.{metric}\",\n",
        "#         window=100,\n",
        "#         title_prefix=f\"Model: {model}\",\n",
        "#         figsize=(12, 6),\n",
        "#         first_n=None,\n",
        "#         y_log_scale=True,\n",
        "#     )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f06e7eb",
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plot_trend_over_time_multi_models(\n",
        "    all_trials,\n",
        "    y_col=\"metrics.rmse_val\",\n",
        "    window=100,\n",
        "    title_prefix=\"Model: \",\n",
        "    figsize=(7, 3 * 4),\n",
        "    first_n=None,\n",
        "    y_log_scale=True,\n",
        ")\n",
        "\n",
        "fig.savefig(\n",
        "    f\"result_figures/trend_over_time_{usecase}_{sampler}.pdf\",\n",
        "    bbox_inches=\"tight\",\n",
        "    dpi=dpi,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8319796a",
      "metadata": {},
      "source": [
        "# Top 1 trial\n",
        "\n",
        "based on held-out test set performance - that's why we're selecting the best_model_type here!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "798cc0b8",
      "metadata": {},
      "outputs": [],
      "source": [
        "top_1_trial = all_trials.loc[all_trials[\"params.model\"] == best_model_type, :].head(1)\n",
        "\n",
        "top_1_trial[\"metrics.nb_features\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b26d20c",
      "metadata": {},
      "outputs": [],
      "source": [
        "top_1_trial[\"tags.experiment_tag\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2afac5b2",
      "metadata": {},
      "outputs": [],
      "source": [
        "top_1_trial_true = all_trials.head(10)\n",
        "top_1_trial_true[\"tags.experiment_tag\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a3bca96",
      "metadata": {},
      "outputs": [],
      "source": [
        "top_1_trial_true[\"metrics.rmse_val\"]"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ritme_model",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
