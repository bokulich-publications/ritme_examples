{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e8241c71",
      "metadata": {},
      "source": [
        "# Evaluate *ritme* trials of all usecases\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1aea4401",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b08cb4f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from src.evaluate_trials import (\n",
        "    multi_boxplot_metric,\n",
        "    plot_complexity_vs_metric,\n",
        "    plot_trend_over_time_multi_models,\n",
        "    plot_recent_param_cat_over_time,\n",
        "    plot_recent_param_cont_over_time,\n",
        ")\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "361c1df1",
      "metadata": {},
      "outputs": [],
      "source": [
        "######## USER INPUTS ########\n",
        "\n",
        "# path to extracted MLflow logs - with script extract_all_logs.sh\n",
        "log_folder_location = \"merged_all_trials_v124.csv\"\n",
        "\n",
        "# which usecase to analyze: \"u1\", \"u2\", \"u3\" or \"all\"\n",
        "usecase = \"u1\"\n",
        "\n",
        "# which samplers to analyse: \"tpe\", \"random\"\n",
        "sampler = \"tpe\"\n",
        "\n",
        "# how many trials to consider for complexity vs. performance plot\n",
        "top_x = 1000\n",
        "\n",
        "# whether to save figures in this run\n",
        "save_figures = False\n",
        "# figure saving dpi\n",
        "dpi = 400\n",
        "######## END USER INPUTS #####"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31db9eea",
      "metadata": {},
      "outputs": [],
      "source": [
        "# set title\n",
        "if usecase == \"u1\":\n",
        "    title = \"Usecase 1\"\n",
        "    log_x_scale = False\n",
        "elif usecase == \"u2\":\n",
        "    title = \"Usecase 2\"\n",
        "    log_x_scale = False\n",
        "elif usecase == \"u3\":\n",
        "    title = \"Usecase 3\"\n",
        "    log_x_scale = True\n",
        "else:\n",
        "    title = \"All usecases\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8390260",
      "metadata": {},
      "source": [
        "## Extract trial information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3347709",
      "metadata": {},
      "outputs": [],
      "source": [
        "# extract all trial information\n",
        "all_trials = pd.read_csv(log_folder_location)\n",
        "# sort by asc metrics.rmse_val\n",
        "all_trials = all_trials.sort_values(by=\"metrics.rmse_val\", ascending=True)\n",
        "print(f\"Found {all_trials.shape[0]} trials\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56e806e7",
      "metadata": {},
      "outputs": [],
      "source": [
        "if usecase != \"all\":\n",
        "    print(f\"Analyzing trials for usecase: {usecase}\")\n",
        "    if usecase == \"u3\":\n",
        "        all_trials = all_trials[\n",
        "            np.logical_and(\n",
        "                all_trials[\"tags.experiment_tag\"].str.startswith(\"u3_galaxy\"),\n",
        "                ~all_trials[\"tags.experiment_tag\"].str.contains(\"w_start\"),\n",
        "            )\n",
        "        ]\n",
        "    else:\n",
        "        all_trials = all_trials[\n",
        "            all_trials[\"tags.experiment_tag\"].str.startswith(usecase)\n",
        "        ]\n",
        "\n",
        "if sampler != \"all\":\n",
        "    print(f\"Analyzing trials for sampler: {sampler}\")\n",
        "    all_trials = all_trials[all_trials[\"tags.experiment_tag\"].str.contains(sampler)]\n",
        "\n",
        "print(f\"Selected {all_trials.shape[0]} trials\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22836d8e",
      "metadata": {},
      "source": [
        "## Find best trial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72801522",
      "metadata": {},
      "outputs": [],
      "source": [
        "# find the best trial & the best model type\n",
        "top_1_trial = all_trials.head(1)\n",
        "best_model_type = top_1_trial[\"params.model\"].values[0]\n",
        "\n",
        "print(best_model_type)\n",
        "top_1_trial[\"tags.experiment_tag\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5176678",
      "metadata": {},
      "outputs": [],
      "source": [
        "top_1_trial[\"metrics.nb_features\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e01f375",
      "metadata": {},
      "source": [
        "## Insights on performance: ALL trials - ft_eng/model vs. RMSE validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a9e4153",
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = multi_boxplot_metric(\n",
        "    all_trials,\n",
        "    metric_col=\"metrics.rmse_val\",\n",
        "    metric_name=\"RMSE Validation\",\n",
        "    group_specs=[\n",
        "        (\"params.data_aggregation\", \"Data aggregation\"),\n",
        "        (\"params.data_selection\", \"Data selection\"),\n",
        "        (\"params.data_transform\", \"Data transform\"),\n",
        "        (\"params.data_enrich\", \"Data enrichment\"),\n",
        "        (\"params.model\", \"Model type\"),\n",
        "    ],\n",
        "    order_by_median=True,\n",
        "    showfliers=False,\n",
        "    title=title,\n",
        "    x_log_scale=log_x_scale,\n",
        ")\n",
        "if save_figures:\n",
        "    fig.savefig(\n",
        "        f\"result_figures/boxplot_all_trials_{usecase}_{sampler}.pdf\",\n",
        "        bbox_inches=\"tight\",\n",
        "        dpi=dpi,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4128255c",
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = multi_boxplot_metric(\n",
        "    all_trials,\n",
        "    metric_col=\"metrics.nb_features\",\n",
        "    metric_name=\"Number of features\",\n",
        "    group_specs=[\n",
        "        (\"params.model\", \"Model type\"),\n",
        "    ],\n",
        "    order_by_median=True,\n",
        "    showfliers=False,\n",
        "    title=title,\n",
        "    x_log_scale=True,\n",
        "    figsize=(6, 4),\n",
        ")\n",
        "plt.tight_layout()\n",
        "if save_figures:\n",
        "    fig.savefig(\n",
        "        f\"result_figures/boxplot_all_trials_{usecase}_{sampler}_nb_fts.pdf\",\n",
        "        bbox_inches=\"tight\",\n",
        "        dpi=dpi,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91e71069",
      "metadata": {},
      "source": [
        "## Model complexity vs. performance: top X trials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed85e8a6",
      "metadata": {},
      "outputs": [],
      "source": [
        "top_x_trials = all_trials.head(top_x)\n",
        "\n",
        "figc, _ = plot_complexity_vs_metric(\n",
        "    top_x_trials,\n",
        "    metric_col=\"metrics.rmse_val\",\n",
        "    metric_name=\"RMSE Validation\",\n",
        "    group_col=\"params.model\",\n",
        "    group_name=\"Model type\",\n",
        "    n=top_x,\n",
        "    figsize=(7, 6),\n",
        "    title=title,\n",
        "    x_log_scale=True,\n",
        ")\n",
        "\n",
        "if save_figures:\n",
        "    figc.savefig(\n",
        "        f\"result_figures/complexity_top_trials_{usecase}_{sampler}.pdf\",\n",
        "        bbox_inches=\"tight\",\n",
        "        dpi=dpi,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58f156ae",
      "metadata": {},
      "source": [
        "## Training over time: ALL trials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a298b2df",
      "metadata": {},
      "outputs": [],
      "source": [
        "nb_models = all_trials[\"params.model\"].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f06e7eb",
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plot_trend_over_time_multi_models(\n",
        "    all_trials,\n",
        "    y_col=\"metrics.rmse_val\",\n",
        "    window=100,\n",
        "    title_prefix=\"Model: \",\n",
        "    figsize=(7, 3 * nb_models),\n",
        "    first_n=None,\n",
        "    y_log_scale=True,\n",
        "    std_alpha=0.3,\n",
        ")\n",
        "if save_figures:\n",
        "    fig.savefig(\n",
        "        f\"result_figures/trend_over_time_{usecase}_{sampler}.pdf\",\n",
        "        bbox_inches=\"tight\",\n",
        "        dpi=dpi,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e861df09",
      "metadata": {},
      "source": [
        "## Trend over time: best model type last N trials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d23d902",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show last N trials as a barplot colored by group\n",
        "n_last = 200\n",
        "window_length = 40"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e611162d",
      "metadata": {},
      "outputs": [],
      "source": [
        "best_model_trials = all_trials[all_trials[\"params.model\"] == best_model_type]\n",
        "best_model_trials.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f06c4272",
      "metadata": {},
      "outputs": [],
      "source": [
        "# get continuous and categorical hyperparameter columns\n",
        "cont_cols = best_model_trials.select_dtypes(include=[np.number]).columns.tolist()\n",
        "cont_params_cols = [x for x in cont_cols if x.startswith(\"params\")]\n",
        "\n",
        "\n",
        "cat_cols = [c for c in best_model_trials.columns if c not in cont_cols]\n",
        "cat_params_cols = [x for x in cat_cols if x.startswith(\"params\")]\n",
        "cat_params_cols.remove(\"params.model\")\n",
        "cat_params_cols.remove(\"params.data_enrich_with\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acf66511",
      "metadata": {},
      "outputs": [],
      "source": [
        "# select columns that are not NaN over all trials of the best_model_types\n",
        "cont_params_cols = [x for x in cont_params_cols if best_model_trials[x].notna().any()]\n",
        "cat_params_cols = [x for x in cat_params_cols if best_model_trials[x].notna().any()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63d7989f",
      "metadata": {},
      "outputs": [],
      "source": [
        "for group in cat_params_cols:\n",
        "    fig_recent, ax_recent = plot_recent_param_cat_over_time(\n",
        "        best_model_trials,\n",
        "        y_col=\"metrics.rmse_val\",\n",
        "        group_col=group,\n",
        "        time_col=\"start_time\",\n",
        "        n_last=n_last,\n",
        "        title=f\"{best_model_type} model \u2014 {group.replace('params.', '').replace('_', ' ')}\",\n",
        "        figsize=(7, 2),\n",
        "        font_scale=0.9,\n",
        "        y_log_scale=False,\n",
        "        window=window_length,\n",
        "    )\n",
        "    if save_figures:\n",
        "        fig_recent.savefig(\n",
        "            f\"result_figures/recent_{n_last}_trials_{usecase}_{sampler}_by_{group.replace('.', '_')}.pdf\",\n",
        "            bbox_inches=\"tight\",\n",
        "            dpi=dpi,\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63d7989f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# logarithmic hyperparameters:\n",
        "log_hyperparams = [\n",
        "    \"params.alpha\",\n",
        "    \"params.min_samples_split\",\n",
        "    \"params.min_samples_leaf\",\n",
        "    \"params.lambda\",\n",
        "    \"params.eta\",\n",
        "    \"params.gamma\",\n",
        "    \"params.reg_alpha\",\n",
        "    \"params.reg_lambda\",\n",
        "    \"params.weight_decay\",\n",
        "    \"params.early_stopping_min_delta\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b67a255",
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot numeric values\n",
        "for param in cont_params_cols:\n",
        "    print(f\"Processing parameter: {param}\")\n",
        "\n",
        "    if param in log_hyperparams:\n",
        "        binning = \"log-uniform\"\n",
        "    else:\n",
        "        binning = \"uniform\"\n",
        "    figc, _ = plot_recent_param_cont_over_time(\n",
        "        best_model_trials,\n",
        "        param_col=param,\n",
        "        group_col=\"params.data_selection\",\n",
        "        time_col=\"start_time\",\n",
        "        n_last=n_last,\n",
        "        n_bins=4,\n",
        "        binning=binning,\n",
        "        figsize=(7, 2),\n",
        "        title=f\"{best_model_type} model \u2014 {param.replace('params.', '').replace('_', ' ')}\",\n",
        "        font_scale=0.9,\n",
        "        palette_name=\"Spectral\",\n",
        "        y_log_scale=False,\n",
        "        window=window_length,\n",
        "    )\n",
        "    if save_figures:\n",
        "        figc.savefig(\n",
        "            f\"result_figures/param_bins_last_{n_last}_{usecase}_{sampler}_{param.replace('.', '_')}.pdf\",\n",
        "            bbox_inches=\"tight\",\n",
        "            dpi=dpi,\n",
        "        )"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ritme_model",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
