{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Usecase 1: Age prediction data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook prepares the dataset for the age prediction usecase following the general data preparation approach outlined in [the original publication by Subramanian et al. 2014](https://doi.org/10.1038/nature13421). It can be run in the following conda environment:\n",
        "```shell\n",
        "mamba env create -f environment_prep_data.yml\n",
        "conda activate ritme_examples_prep_data\n",
        "pip install -e .\n",
        "qiime dev refresh-cache\n",
        "```\n",
        "\n",
        "Additionally, you must run the `vdb-config` tool and exit by pressing x (needed to initialize the wrapped SRA Toolkit for more information see [here](https://github.com/ncbi/sra-tools/wiki/05.-Toolkit-Configuration))\n",
        "```shell\n",
        "vdb-config -i\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import qiime2 as q2\n",
        "from qiime2.plugins import demux\n",
        "\n",
        "import src.meta_proc_subr14 as proc_subr\n",
        "from src.meta_fetch import _fetch_all_supp_material, _fetch_sra_metadata, save_file\n",
        "from src.seq_fetch_n_process import (\n",
        "    cluster_wq_sequences,\n",
        "    create_phylogeny,\n",
        "    create_taxonomy,\n",
        "    fetch_sequences,\n",
        "    filter_sequences,\n",
        "    rarefy_sequences_w_fixed_seed,\n",
        ")\n",
        "from src.seq_trim import trim_sequences\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "######## USER INPUTS ########\n",
        "bioproject_id = \"PRJEB5482\"\n",
        "path_to_data = \"../../data/u1_subramanian14\"\n",
        "email = \"my@mail.com\"\n",
        "n_jobs = 6\n",
        "tag = \"01\"\n",
        "seed = 148\n",
        "######## END USER INPUTS #####"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not os.path.exists(path_to_data):\n",
        "    os.makedirs(path_to_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fetch and process metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# fetch SRA metadata (takes ~3 min)\n",
        "sra_ids = pd.Series([bioproject_id], name=\"ID\")\n",
        "ids = q2.Artifact.import_data(\"NCBIAccessionIDs\", sra_ids)\n",
        "\n",
        "md_sra = _fetch_sra_metadata(path_to_data, ids, email, n_jobs)\n",
        "md_sra = proc_subr._process_sra_metadata(md_sra)\n",
        "\n",
        "# fetch supp. material\n",
        "url_supp = (\n",
        "    \"https://static-content.springer.com/esm/\"\n",
        "    \"art%3A10.1038%2Fnature13421/MediaObjects/\"\n",
        "    \"41586_2014_BFnature13421_MOESM97_ESM.xlsx\"\n",
        ")\n",
        "path2supp = _fetch_all_supp_material(path_to_data, url_supp)\n",
        "md_supp = proc_subr.process_supp_metadata(path2supp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# merge\n",
        "md_all = md_sra.merge(md_supp, how=\"left\", on=\"sample_id\")\n",
        "md_all = proc_subr._postprocess_all_metadata(md_all)\n",
        "\n",
        "# save to file\n",
        "path_to_md = save_file(md_all, path_to_data, tag)\n",
        "\n",
        "# get number of samples\n",
        "nb_samples = md_all.shape[0]\n",
        "\n",
        "print(md_all.shape)\n",
        "md_all.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fetch and process sequences\n",
        "--> output after 5: `data/u1_subramanian14/otu_table_subr14_rar.tsv`        \n",
        "--> output after 3: `data/u1_subramanian14/otu_table_subr14_wq.qza`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Following the general approach outlined in [the original publication by Subramanian et al. 2014](https://doi.org/10.1038/nature13421), namely:\n",
        "1) fetching sequences from NCBI SRA\n",
        "2) trim sequences to at most 162 nucleotide length and overlap forward and reverse reads\n",
        "3) clustering sequences sharing >= 97% identity matched to the 13_8 99% Greengenes reference and remaining sequences were clustered de novo\n",
        "    * creating taxonomy and phylogeny for all OTUs        \n",
        "**--> feature table used by *ritme***\n",
        "4) filtering such that only OTUs present at or above a level of confident detection (0.1% relative abundance) in at least two fecal samples.\n",
        "5) rarefaction of resulting OTU table at 2'000 sequences per sample         \n",
        "    **--> feature table used by original publication**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1. fetch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fetch_sequences(n_jobs, path_to_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# check size\n",
        "path_to_paired = os.path.join(path_to_data, \"PRJEB5482\", \"paired_reads.qza\")\n",
        "paired_reads = q2.Artifact.load(os.path.join(path_to_paired))\n",
        "\n",
        "# summarize\n",
        "path_summary = path_to_paired.replace(\".qza\", \"_summary.qzv\")\n",
        "if not os.path.isfile(path_summary):\n",
        "    (sum_paired,) = demux.actions.summarize(data=paired_reads)\n",
        "    sum_paired.save(path_summary)\n",
        "    print(f\"Saved paired summary in: {path_summary}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "448 samples with 18858.3 median forward and reverse reads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2. trim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trim_sequences(path2md=path_to_md, path2seq=path_to_data, threads=n_jobs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# check size\n",
        "path_to_trim = os.path.join(path_to_data, \"trimmed_subramanian14.qza\")\n",
        "trimmed_reads = q2.Artifact.load(os.path.join(path_to_trim))\n",
        "\n",
        "# summarize\n",
        "path_summary = path_to_trim.replace(\".qza\", \"_summary.qzv\")\n",
        "\n",
        "if not os.path.isfile(path_summary):\n",
        "    (sum_reads,) = demux.actions.summarize(data=trimmed_reads)\n",
        "    sum_reads.save(path_summary)\n",
        "    print(f\"Saved demux summary in: {path_summary}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "448 samples with 18349.5 median forward and reverse reads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3. cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cluster_wq_sequences(path_to_data=path_to_data, n_threads=n_jobs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# check size\n",
        "path_to_otu = os.path.join(path_to_data, \"otu_table_subr14_wq.qza\")\n",
        "otu_table = q2.Artifact.load(os.path.join(path_to_otu))\n",
        "out_table_df = otu_table.view(pd.DataFrame)\n",
        "out_table_df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Create taxonomy & phylogeny"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "create_taxonomy(path_to_data=path_to_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "create_phylogeny(path_to_data=path_to_data, n_threads=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4. filter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "filter_sequences(path_to_data=path_to_data, min_prevalence=2 / nb_samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# check size\n",
        "path_to_otu = os.path.join(path_to_data, \"otu_table_subr14_filt.qza\")\n",
        "otu_table = q2.Artifact.load(os.path.join(path_to_otu))\n",
        "otu_table.view(pd.DataFrame).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 5. rarefy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rarefied_table = rarefy_sequences_w_fixed_seed(path_to_otu=path_to_otu, seed=seed)\n",
        "\n",
        "# assert that rarefaction worked\n",
        "assert np.unique(rarefied_table.sum(axis=\"sample\"))[0] == 2000\n",
        "\n",
        "# save to file\n",
        "path_to_rar = os.path.join(path_to_data, \"otu_table_subr14_rar.tsv\")\n",
        "df_rarefied_table = rarefied_table.to_dataframe().transpose()\n",
        "df_rarefied_table.to_csv(path_to_rar, sep=\"\\t\")\n",
        "\n",
        "df_rarefied_table.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "number of samples here: (448, 850)\n",
        "\n",
        "[note: denoising would yield even less: (333, 679)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Subset metadata by samples in feature tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# both feature tables have the same samples within\n",
        "assert df_rarefied_table.index.tolist() == out_table_df.index.tolist()\n",
        "\n",
        "print(md_all.shape)\n",
        "md_subset = md_all.loc[df_rarefied_table.index]\n",
        "\n",
        "path_to_md = os.path.join(path_to_data, \"md_subr14.tsv\")\n",
        "md_subset.to_csv(path_to_md, sep=\"\\t\")\n",
        "md_subset.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Describe metadata and sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "md_subset.study_subcohort.value_counts(dropna=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "md_subset.host_id.nunique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This study did not upload all sequences needed to reproduce their original RF training and testing, that would result in 50 unique infants. In the study's original train set, 12 healthy infants with 1,222 97%-identity OTUs were used to train the model and 25 twins & triplets and 13 singletons were then used to test it. However, only a fraction (25 out of 38 infants) from the test set were actually uploaded to SRA.\n",
        "\n",
        "With this we have 25 infants with 448 samples and 888 cleaned OTUs for modelling."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ritme_examples_prep_data",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
